\chapter{Introduction and Background Research}

% You can cite chapters by using '\ref{chapter1}', where the label must
% match that given in the 'label' command, as on the next line.
\label{chapter1}

% Sections and sub-sections can be declared using \section and \subsection.
% There is also a \subsubsection, but consider carefully if you really need
% so many layers of section structure.
\section{Introduction}

Algorithms such as Deep Blue \cite{CAMPBELL200257}, AlphaZero \cite{silver2018general} and more recently Player of Games\cite{schmid2021player}
have enabled computers to out smart the smartest humans at the game of Chess.
Unfortunately all these algorithms are bound to the digital world, rendered useless when
competing against humans on a real board.  This project aims to explore a major component of this: vision.

Unlike humans, the hard part of chess for computer is not planning which move to take next, but instead recognition, 
localisation and manipulation of objects in 3D space which currently all present the much greater challenges.
Perhaps the reason for the vision problem feeling so apparently effortless to humans is that over half of the human cortex is allocated 
to visual processing \cite{snowden2012basic}.
It is also described by Szeliski as an inverse problem and to attribute that as the reason for it's difficulty
within computer science \cite{szeliski2011computer}.

Consider the vision problem for chess to be two-fold: what is the current board state and where are all of the pieces?  
In particular this project will focus on the former, that is, to produce and present a solution for determining the
state of a chess board from a video stream.  A solution reliable enough to live up to the likes of AlphaZero 
in a robotic system,
but also a solution that could be immediately useful in other applications such as realtime chess analysis from a real board.

There will be a focus on deep learning techniques, with consideration for best practice and the aim to share the 
tools to more easily manage and create new datasets in this area.  Something 
called for by \cite{Ding2016ChessVisionC} as a serious challenge and priority for future research.


\section{Literature Review}
\label{research}


\subsection{A \textit{Brief} History of Computer Vision}
\label{a breif history}
Computer vision is the study of making sense from visual data.  Applications include character recognition for digitising documents, segmenting and classifying 
object instances for cancer screening, object tracking for sports analysis from video and countless more. 
To reach these goals, we name \textit{features} to be useful pieces of information within an image (or any other visual data) and \textit{feature detection} to be
the class of algorithms that can extract features from images.

Some of the earliest work in computer vision started with Larry Robert in his 1965 paper \cite{} describing a simple 2x2 convolutional
kernel for edge detection which soon became the predecessor to the Sobel operator in 1968 \cite{} and the still widely used
Canny edge detector developed in 1986 \cite{}. The ability to find edges provided a helpful backbone for detection of higher order features such as lines and corners. 

Another one of these tools that is still commonly used to help more sophisticated feature extractors is \textit{thresholding} which refers to the process of 
partitioning images into two sub regions based on a pixel color/intensity threshold value.  
More complex partitioning schemes are often referred to as image segmentation techniques.
Otsu's method is a famous example of an automatic global thresholding algorithm that finds a suitable threshold value for the entire image without supervision
by minimising the resulting variance both between and within the two separated sub regions \cite{}.

For higher order features more sophisticated methods are needed, a bucket term used to describe some of these methods is \textit{template matching}, which can generally be 
considered as comparing an image to known feature templates to determine if that feature is present\footnote{Neural networks perform template matching 
on an input against their learned internal features}.  The Hough transform is the one of these higher level feature extractors commonly used to extract lines, circles 
and even arbitrary shapes, which heavily relies of edge detection \cite{houghtransform}.

Feature Descriptors (keypoint, intest detectors) ... Laplacian, Harris, SIFT, SURF, ORB, HOG
% https://web.stanford.edu/class/ee368/Handouts/Lectures/2014_Spring/Combined_Slides/12-Keypoint-Detection-Combined.pdf


\subsubsection{Neural Networks}
Enter...  Neural Networks now do all of the above, better.
Brief overview of some of the big ones then focus in on a few specific architectures for image classification used throught the project

% \textbf{Image Recognition}
% the way back in 1980 \cite{} convolutions showed promise in simple computer vision tasks, 
% convolutions since have showed extreme promise \cite{}
% Le Ye Cunn's work with convolutions \cite{} and MNIST \cite{} and more recently ImageNet \cite{} having become incredibly well known.
% New methods such as Transformers from the world have NLP have generalised the convolution operation have proved 
% very successful and lots of work here has been applied to vision. \cite{} \cite{} \cite{} (Attention is all you need, ViT, generic model from deepmind)
% \textbf{Object Detection}
% But in most applications there will be many things we want to recognise in an image.  The RCNN \cite{} enters.  How Faster-RCNN improves on this \cite{}.
% Why YOLO \cite{} has been so successful (realtime).  Transformers are applicable here too.
% \textbf{Instance Segmentation}
% Why bounding boxes are not enough.  What is segmentation? Why instance segmentation is what we actually want. \cite{}
% \textbf{Adding More Dimensions}
% The real world is not percepived in static 2d images.  How do we add an understanding of 3 dimensions and time in our computer vision models? \cite{}
% Important for localisation in the real world.  Important for understanding things like object permenance.


\subsection{Computer Vision for Chess}
Despite chess being a very narrow application of computer vision, the amount of research effort gone into the problem of determining 
board state is not insignificant. 
A variety of approaches have been tried and tested for which the following section will now summarise.

As in \cite{Ding2016ChessVisionC} the vision problem can be split into two further problems for analysis: board detection and piece recognition.

\subsubsection{Board Detection}
The problem of board detection is not specific to chess but also receives heavy research from other applications such as camera calibration \cite{cameraCalibration}. 
The built in camera calibration functions in opencv \cite{opencv_library} and matlab \cite{MATLAB:2010} are used in many previous works \cite{Koray2016ACV, bowers_2014} which 
provide a quick and precise solution for board detection, but becomes unusable when any artifacts like chess pieces are present on the board.  
This forced those authors to take the approach of an initial setup stage at inference, making the solution unfit to changes in board position during inference. 

Due to a chessboard's simple features many early works of line and corner point detection can be applied.  For example Hough 
transforms are used to detect the lines of a chessboard \cite{CVChess, nusChessVision}.  Corner point detection methods such as the 
Harris and Stephens's \cite{} were also common among solutions \cite{}, with some authors combining approaches with further 
processing such as canny edge detection \cite{} to yield more reliable results.

ChESS was another corner detection algorithm that out performed the Harris and Stephen's algorithm \cite{}. This was, perhaps 
interestingly, created for real-time measurement of lung function in humans, further demonstrating the attention chess board
detection has received due to its general applicability.

There are many other algorithms that require simplifications such as custom green and red chessboards \cite{}, multiple camera angles \cite{}, 
or even the requirement of user input for entering the corners of the chessboard \cite{}.

The most impressive work came out of Poznan University of Technology which proposes many interesting ideas that perform more 
reliably in a wider range of difficult situation such as pieces being present on the board \cite{}.  They employ an iterative 
approach with each iteration containing 3 sub-tasks: line segment detection, lattice point search and chessboard position search.
In each iteration of line detection a canny lines detector \cite{} is used on many preprocessed variations of the input image 
to maximize the number of relevant line detections which are then merged using a linking function. The lattice point search starts with 
the intersection of all merged lines as input, converting these intersection points to a 21x21 pixel binary image of the surrounding area and 
runs them through a classifier to remove outliers.  The addition of a neural network as a classifier greatly improves 
the generality of the proposed solution as it can be resistant to lattice points that are partially covered by a chess piece.
The final sub-task then creates a heatmap over the original image representing the likelihood of a chessboard being present.  
Under the hood this is done by calculating a polyscore for the set of most likely quadrilaterals formed by the lines of the first stage.
The polyscore is a function of the area of the quadrilateral and the lattice points contained within it.
It is the quadrilateral that produces the highest polyscore that is used to segment the image for input to the next iteration until the quadrilateral
points converge.  The main disadvantage of this approach compared to others is that it can take up to 5 seconds to process one frame, 
for most use cases however this will be sufficient, unless realtime board training is required.

\subsubsection{Piece Recognition}

Piece recognition has proved more difficult \cite{}.  Most chess vision systems avoid classifying pieces by 
type (knight, king, ect.) all together \cite{}.  These approaches typically get around this by requiring the board to start in a known position.
From this known state the normal rules of chess can be used to infer what pieces are where after each move.  Simpler methods
require human input to prompt when a move has been taken \cite{}, more sophisticated attempt to do this move detection automatically.

These automatic move detection methods tend to all follow the same overarching processes of thresholding to detect a move having occurred, 
whether on color \cite{} or even the edges detected within each square \cite{}.  
Most authors recognise the dependance this approach has on lighting variations, with Otsu thresholding \cite{} sometimes being used to minimise 
the negative impact when lighting changed.  While this improved results for what may be considered normal lighting conditions, they still suffered.
They calculate reference colors for all 4 variations (white square, black square, white piece, black piece).  
All of these then only work in situation where a series of moves are to be recorded according to typical chess rules and not the chessboard state at any given moment.

There were a couple of methods that stood out from the rest each in their own way.  One used fourier descriptors to model piece types from a training set
and the other modelled the pieces in Blender, a 3D modelling software, utilising template matching to determine piece type.  The fourier method was very sensitive 
to change in camera angles, preferring a side view angle that unfortunately cause too many occlusions to be practical.  The template matching approach
took over 150 seconds on average to predict board state from one image which does not lend itself to interactive play in a robotic environment.

Go to standford dude and the heatmap guys as the best approach out there. They use SIFT and hard coded color alogrithms.  heatmap guys improved on this 
only by adding more restrictions by assuming the board much be valid and making statistical assumptions on 
what state is most likely.  Oh and a HOG method.  The one that said SIFT didn't work well because the lack of texture.

More recently another group of methods have surfaced using neural networks, specifically convolutional neural networks (CNNs) \cite{}.
One of these used a pretrained Inception-ResNet-v2 model \cite{} and only had 6 classes, resorting to the more tradition approaches for color detection,
in particular binary thresholding with added morphological transformations to reduce noise as seen in previous works \cite{}.  
Interestingly the six chosen classes were 'empty', 'pawn', 'knight', 'bishop', 'rook' and 'king\_or\_queen' as they claim kings and queens can be 
difficult even for human eyes to distinguish.  Because of the choice of classes this method falls back to relying on a chess engine to determine piece type,
which while usually correct for normal games of play makes the method unusable for games played with a variation on the normal rules of play.
The other two methods used a simpler CNN structure similar to that of VGG \cite{} with 13 classes, one for every colored piece as well as the empty square.


\subsection{Prior Work From the Author}
Mention robotic arm and vision system for two counter board games as well as automatic differentiation library.
